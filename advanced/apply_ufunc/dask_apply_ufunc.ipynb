{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Handling dask arrays\n",
    "\n",
    "We have previously worked over applying functions to NumPy arrays contained in Xarray objects.\n",
    "`apply_ufunc` also lets you easily perform many of the steps involving in applying \n",
    "functions that expect and return Dask arrays.\n",
    "\n",
    "Learning goals:\n",
    "- Learn that `apply_ufunc` can automate aspects of applying computation functions on dask arrays\n",
    "- It is possible to automatically parallelize certain operations by providing `dask=\"parallelized\"`\n",
    "- In some cases, extra information needs to be provided such as sizes of any new dimensions added, or data types for output variables.\n",
    "- Learn that all the concepts from the numpy lessons carry over: like [automatic vectorization](vectorize) and specifying input and\n",
    "  output core dimensions.\n",
    "\n",
    "\n",
    "```{tip}\n",
    "We'll reduce the length of error messages using `%xmode minimal` See the [ipython documentation](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-xmode) for details.\n",
    "```\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "\n",
    "import dask\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# limit the amount of information printed to screen\n",
    "xr.set_options(display_expand_data=False)\n",
    "np.set_printoptions(threshold=10, edgeitems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "First lets set up a `LocalCluster` using [dask.distributed](https://distributed.dask.org/).\n",
    "\n",
    "You can use any kind of dask cluster. This step is completely independent of\n",
    "xarray. While not strictly necessary, the dashboard provides a nice learning\n",
    "tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "<p>&#128070</p> Click the Dashboard link above. Or click the \"Search\" button in the dashboard.\n",
    "\n",
    "Let's test that the dashboard is working..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array\n",
    "\n",
    "dask.array.ones((1000, 4), chunks=(2, 1)).compute()  # should see activity in dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Let's open a dataset. We specify `chunks` so that we create a dask arrays for the DataArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.tutorial.open_dataset(\"air_temperature\", chunks={\"time\": 100})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A simple example\n",
    "\n",
    "All the concepts from applying numpy functions carry over.\n",
    "\n",
    "However the handling of dask arrays needs to be explicitly activated.\n",
    "\n",
    "There are three options for the `dask` kwarg.\n",
    "\n",
    "```\n",
    "    dask : {\"forbidden\", \"allowed\", \"parallelized\"}, default: \"forbidden\"\n",
    "        How to handle applying to objects containing lazy data in the form of\n",
    "        dask arrays:\n",
    "\n",
    "        - 'forbidden' (default): raise an error if a dask array is encountered.\n",
    "        - 'allowed': pass dask arrays directly on to ``func``. Prefer this option if\n",
    "          ``func`` natively supports dask arrays.\n",
    "        - 'parallelized': automatically parallelize ``func`` if any of the\n",
    "          inputs are a dask array by using :py:func:`dask.array.apply_gufunc`. Multiple output\n",
    "          arguments are supported. Only use this option if ``func`` does not natively\n",
    "          support dask arrays (e.g. converts them to numpy arrays).\n",
    "```\n",
    "\n",
    "We will work through the following two:\n",
    "\n",
    "1. `dask=\"allowed\"` Dask arrays are passed to the user function. This is a good\n",
    "   choice if your function can handle dask arrays and won't compute the result unless \n",
    "   explicitly requested.\n",
    "2. `dask=\"parallelized\"`. This applies the user function over blocks of the dask\n",
    "   array using `dask.array.apply_gufunc`. This is useful when your function cannot\n",
    "   handle dask arrays natively (e.g. scipy API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Expect an error here\n",
    "def squared_error(x, y):\n",
    "    return (x - y) ** 2\n",
    "\n",
    "\n",
    "xr.apply_ufunc(squared_error, ds.air, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "tags": []
   },
   "source": [
    "  \n",
    "A good thing to check is whether the applied function (here `squared_error`) can handle pure dask arrays. \n",
    "To do this call  `squared_error(ds.air.data, 1)` and make sure of the following:\n",
    "1. That you don't see any activity on the dask dashboard\n",
    "2. That the returned result is a dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "squared_error(ds.air.data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since `squared_error` can handle dask arrays without computing them, we specify\n",
    "`dask=\"allowed\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqer = xr.apply_ufunc(\n",
    "    squared_error,\n",
    "    ds.air,\n",
    "    1,\n",
    "    dask=\"allowed\",\n",
    ")\n",
    "sqer  # dask-backed DataArray! with nice metadata!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding what's happening\n",
    "\n",
    "Let's again use the wrapper trick to understand what `squared_error` receives.\n",
    "\n",
    "We see that it receives a dask array (analogous to the numpy array in the previous example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(x, y):\n",
    "    print(f\"received x of type {type(x)}, shape {x.shape}\")\n",
    "    print(f\"received y of type {type(y)}\")\n",
    "    return squared_error(x, y)\n",
    "\n",
    "\n",
    "xr.apply_ufunc(wrapper, ds.air, 1, dask=\"allowed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Core dimensions\n",
    "\n",
    "`squared_error` operated on a per-element basis. How about a reduction like `np.mean`?\n",
    "\n",
    "Such functions involve the concept of \"core dimensions\". This concept is independent of the underlying array type, and is a property of the applied function. See the [core dimensions with NumPy](core-dimensions) tutorial for more.\n",
    "\n",
    "\n",
    "```{exercise}\n",
    ":label: daskmean\n",
    "\n",
    "Use `dask.array.mean` as an example of a function that can handle dask\n",
    "arrays and uses an `axis` kwarg. \n",
    "```\n",
    "\n",
    "````{solution} daskmean\n",
    ":class: dropdown\n",
    "```python\n",
    "def time_mean(da):\n",
    "    return xr.apply_ufunc(\n",
    "        dask.array.mean,\n",
    "        da,\n",
    "        input_core_dims=[[\"time\"]],\n",
    "        dask=\"allowed\",\n",
    "        kwargs={\"axis\": -1},  # core dimensions are moved to the end\n",
    "    )\n",
    "\n",
    "\n",
    "time_mean(ds.air)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "tags": []
   },
   "source": [
    "Again, this is identical to the built-in `mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_mean(da):\n",
    "    return xr.apply_ufunc(\n",
    "        dask.array.mean,\n",
    "        da,\n",
    "        input_core_dims=[[\"time\"]],\n",
    "        dask=\"allowed\",\n",
    "        kwargs={\"axis\": -1},  # core dimensions are moved to the end\n",
    "    )\n",
    "\n",
    "\n",
    "ds.air.mean(\"time\").identical(time_mean(ds.air))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Automatically parallelizing dask-unaware functions\n",
    "\n",
    "### Basics\n",
    "\n",
    "Not all functions can handle dask arrays appropriately by default.\n",
    "\n",
    "A very useful `apply_ufunc` feature is the ability to apply arbitrary functions\n",
    "in parallel to each block. This ability can be activated using\n",
    "`dask=\"parallelized\"`. \n",
    "\n",
    "We will use `scipy.integrate.trapz` as an example of a function that cannot\n",
    "handle dask arrays and requires a core dimension. If we call `trapz` with a dask\n",
    "array, we get a numpy array back that is, the values have been eagerly computed.\n",
    "This is undesirable behaviour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.integrate\n",
    "\n",
    "sp.integrate.trapz(\n",
    "    ds.air.data, axis=ds.air.get_axis_num(\"lon\")\n",
    ")  # does NOT return a dask array, you should see activity on the dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's activate automatic parallelization by using `apply_ufunc` with `dask=\"parallelized\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated = xr.apply_ufunc(\n",
    "    sp.integrate.trapz,\n",
    "    ds,\n",
    "    input_core_dims=[[\"lon\"]],\n",
    "    kwargs={\"axis\": -1},\n",
    "    dask=\"parallelized\",\n",
    ")\n",
    "integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "tags": []
   },
   "source": [
    "And make sure the returned data is a dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integrated.air.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Now you have control over executing this parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask -> Numpy array of integrated values\n",
    "parallelized_results = integrated.compute()\n",
    "parallelized_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding `dask=\"parallelized\"`\n",
    "\n",
    "It is very important to understand what `dask=\"parallelized\"` does. To fully understand it, requires understanding some core concepts.\n",
    "\n",
    "```{seealso}\n",
    "For `dask=\"parallelized\"` `apply_ufunc` will call `dask.array.apply_gufunc`. See the dask documentation on [generalized ufuncs](https://docs.dask.org/en/stable/array-gufunc.html) and [`apply_gufunc`](https://docs.dask.org/en/stable/generated/dask.array.gufunc.apply_gufunc.html) for more.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Embarrassingly parallel or blockwise operations\n",
    "\n",
    "`dask=\"parallelized\"` works well for \"blockwise\" or \"embarrassingly parallel\" operations ([Wikipedia](https://en.wikipedia.org/wiki/Embarrassingly_parallel)).\n",
    "\n",
    "These are operations where one block or chunk of the output array corresponds to one block or chunk of the input array. Specifically, the blocks or chunks of the _core dimension_ is what matters. Importantly, no communication between blocks is necessary to create the output, which makes parallelization quite simple or \"embarrassing\".\n",
    "\n",
    "Let's look at the dask repr for `ds` and note chunksizes are (100,25,53) for a array with shape (2920, 25, 53). This means that each block or chunk of the array contains all `lat`, `lon` points and a subset of `time` points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.air.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "tags": []
   },
   "source": [
    "The core dimension for `trapz` is `lon`, and there is only one chunk along `lon`. This means that integrating along `lon` is a \"blockwise\" or \"embarrassingly parallel\" operation and `dask=\"parallelized\"` works quite well. \n",
    "\n",
    "```{caution} Question\n",
    "Do you understand why `integrate(ds)` when `ds` has a single chunk along `lon` is a \"embarrassingly parallel\" operation?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "tags": []
   },
   "source": [
    "```{exercise} \n",
    ":label: rechunk\n",
    "Apply the integrate function to `ds` after rechunking to have a different chunksize along `lon` using `ds.chunk(lon=4)` (for example). What happens?\n",
    "```\n",
    "\n",
    "```{solution} rechunk\n",
    ":class: dropdown\n",
    "\n",
    "`apply_ufunc` complains that it cannot automatically parallelize because the dataset `ds` is now chunked along the core dimension `lon`. You should see the following error:\n",
    "\n",
    "    ValueError: dimension lon on 0th function argument to apply_ufunc with dask='parallelized' \n",
    "    consists of multiple chunks, but is also a core dimension. To fix, either rechunk \n",
    "    into a single array chunk along this dimension, i.e., \n",
    "    ``.chunk(dict(lon=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` \n",
    "    but beware that this may significantly increase memory usage.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Understanding execution\n",
    "\n",
    "We are layering many concepts together there so it is important to understand how the function is executed, and what input it will receive. Again we will use our wrapper trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def integrate_wrapper(array, **kwargs):\n",
    "    print(f\"received array of type {type(array)}, shape {array.shape}\")\n",
    "    result = sp.integrate.trapz(array, **kwargs)\n",
    "    print(f\"received array of type {type(result)}, shape {result.shape}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "integrated = xr.apply_ufunc(\n",
    "    integrate_wrapper,\n",
    "    ds,\n",
    "    input_core_dims=[[\"lon\"]],\n",
    "    kwargs={\"axis\": -1},\n",
    "    dask=\"parallelized\",\n",
    ")\n",
    "integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Note that we received an Xarray object back (`integrated`) but our wrapper function was called with a numpy array of shape `(1,1,1)`.\n",
    "\n",
    "```{important}\n",
    "the full 3D array has **not yet been** passed to `integrate_wrapper`. Yet dask needs to know the shape and dtype of the result. This is key. \n",
    "```\n",
    "\n",
    "The `integrate_wrapper` function is treated like a black box, and its effect on the inputs has to either be described through additional keyword arguments, or inferred by passing dummy inputs.\n",
    "\n",
    "To do so, `dask.array.apply_gufunc` calls the user function with dummy inputs (here a numpy array of shape `(1,1,1)`), and inspects the returned value to understand that one dimension was removed (returned a numpy array of shape `(1,1)`.\n",
    "\n",
    "````{caution}\n",
    ":class: dropdown\n",
    "\n",
    "Some functions can have trouble handling such dummy inputs. Alternatively you can pass `meta = np.ones((1,1))` in `dask_gufunc_kwargs` to prevent dask from providing dummy inputs to the array.\n",
    "```python\n",
    "xr.apply_ufunc(\n",
    "    integrate_wrapper,\n",
    "    ds,\n",
    "    input_core_dims=[[\"lon\"]],\n",
    "    kwargs={\"axis\": -1},\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs={\"meta\": np.ones((1,1))},\n",
    ")\n",
    "```\n",
    "````\n",
    "\n",
    "Since no errors were raised we proceed as-is.\n",
    "\n",
    "Let's compute the array to get real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": [
     "output-scroll"
    ]
   },
   "outputs": [],
   "source": [
    "integrated.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "We see that `integrate_wrapper` is called many times! As many times as there are blocks in the array in fact, which is 30 here (`ds.air.data.numblocks`).\n",
    "\n",
    "Our function is independently executed on each block of the array, and then the results are concatenated to form the final result.\n",
    "\n",
    "Conceptually, there is a two-way flow of information between various packages when executing `integrated.compute()`:\n",
    "\n",
    "`xarray.apply_ufunc` ↔ `dask.array.apply_gufunc` ↔ `integrate_wrapper` ↔ `scipy.integrate.trapz` ↔ `ds.air.data`\n",
    "\n",
    "\n",
    "When executed\n",
    "\n",
    "1. Xarray loops over all data variables.\n",
    "1. Xarray unwraps the underlying dask array (e.g. `ds.air`) and passes that to dask's `apply_gufunc`.\n",
    "1. `apply_gufunc` calls `integrate_wrapper` on each block of the array.\n",
    "1. For each block, `integrate_wrapper` calls `scipy.integrate.trapz` and returns one block of the output array.\n",
    "1. dask stitches all the output blocks to form the output array.\n",
    "1. `xarray.apply_ufunc` wraps the output array with Xarray metadata to give the final result.\n",
    "\n",
    "Phew!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## More complex situations\n",
    "\n",
    "Here we quickly demonstrate that all the concepts from the numpy material earlier carry over.\n",
    "\n",
    "Xarray needs a lot of extra metadata, so depending\n",
    "on the function, extra arguments such as `output_dtypes` and `output_sizes` may\n",
    "be necessary for supporting dask arrays. We demonstrate this below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adding new dimensions\n",
    "\n",
    "We use the [expand_dims example](newdim) that changes the size of the input along a single dimension.\n",
    "\n",
    "```python\n",
    "def add_new_dim(array):\n",
    "    return np.expand_dims(array, axis=0)\n",
    "```\n",
    "\n",
    "When automatically parallelizing with `dask`, we need to provide some more information about the outputs.\n",
    "1. When adding a new dimensions, we need to provide the size in `dask_gufunc_kwargs` using the key `output_sizes`\n",
    "2. Usually we need provide the datatype or `dtype` of the returned array. Usually the dtype of the input is a good guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def add_new_dim(array):\n",
    "    return np.expand_dims(array, axis=-1)\n",
    "\n",
    "\n",
    "xr.apply_ufunc(\n",
    "    add_new_dim,  # first the function\n",
    "    ds.air.chunk({\"time\": 2, \"lon\": 2}),\n",
    "    output_core_dims=[[\"newdim\"]],\n",
    "    dask=\"parallelized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Provide the size of the newly added dimension `newdim` in `output_sizes` as part of the `dask_gufunc_kwargs` keyword argument:\n",
    "\n",
    "    dask_gufunc_kwargs (dict, optional) – Optional keyword arguments passed to dask.array.apply_gufunc() \n",
    "    if dask=’parallelized’. Possible keywords are output_sizes, allow_rechunk and meta.\n",
    "    \n",
    "The syntax is \n",
    "```python\n",
    "dask_gufunc_kwargs={\n",
    "    \"output_sizes\": {\"newdim\": 1}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xr.apply_ufunc(\n",
    "    add_new_dim,  # first the function\n",
    "    ds.air.chunk({\"time\": 2, \"lon\": 2}),\n",
    "    output_core_dims=[[\"newdim\"]],\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs={\"output_sizes\": {\"newdim\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Dimensions that change size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "We will now repeat the [interpolation example from earlier](interp-add-new-dim) with `\"lat\"` as the output core dimension. See the numpy notebook on [complex output](complex-output) for more.\n",
    "\n",
    "```python\n",
    "newlat = np.linspace(15, 75, 100)\n",
    "\n",
    "xr.apply_ufunc(\n",
    "    np.interp,\n",
    "    newlat,\n",
    "    ds.air.lat,\n",
    "    ds.air.chunk({\"time\": 2, \"lon\": 2}),\n",
    "    input_core_dims=[[\"lat\"], [\"lat\"], [\"lat\"]],\n",
    "    output_core_dims=[[\"lat\"]],\n",
    "    exclude_dims={\"lat\"},\n",
    ")\n",
    "```\n",
    "\n",
    "We will first add `dask=\"parallelized\"` and provide `output_sizes` in `dask_gufunc_kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "newlat = np.linspace(15, 75, 100)\n",
    "\n",
    "xr.apply_ufunc(\n",
    "    np.interp,  # first the function\n",
    "    newlat,\n",
    "    ds.air.lat,\n",
    "    ds.air.chunk({\"time\": 2, \"lon\": 2}),\n",
    "    input_core_dims=[[\"lat\"], [\"lat\"], [\"lat\"]],\n",
    "    output_core_dims=[[\"lat\"]],\n",
    "    exclude_dims={\"lat\"},\n",
    "    # The following are dask-specific\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs=dict(output_sizes={\"lat\": len(newlat)}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "This error means that we need to provide `output_dtypes`\n",
    "\n",
    "    output_dtypes (list of dtype, optional) – Optional list of output dtypes. \n",
    "    Only used if dask='parallelized' or vectorize=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newlat = np.linspace(15, 75, 100)\n",
    "\n",
    "xr.apply_ufunc(\n",
    "    np.interp,  # first the function\n",
    "    newlat,\n",
    "    ds.air.lat,\n",
    "    ds.air.chunk({\"time\": 100, \"lon\": -1}),\n",
    "    input_core_dims=[[\"lat\"], [\"lat\"], [\"lat\"]],\n",
    "    output_core_dims=[[\"lat\"]],\n",
    "    exclude_dims={\"lat\"},\n",
    "    # The following are dask-specific\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs=dict(output_sizes={\"lat\": len(newlat)}),\n",
    "    output_dtypes=[ds.air.dtype],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Dask can sometimes figure out the output sizes and dtypes. The usual workflow is to read the error messages and iteratively pass more information to `apply_ufunc`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Automatic Vectorizing\n",
    "\n",
    "[Automatic vectorizing](vectorize) with `vectorize=True` also carries over!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interped = xr.apply_ufunc(\n",
    "    np.interp,  # first the function\n",
    "    newlat,\n",
    "    ds.air.lat,\n",
    "    ds.chunk({\"time\": 100, \"lon\": -1}),\n",
    "    input_core_dims=[[\"lat\"], [\"lat\"], [\"lat\"]],\n",
    "    output_core_dims=[[\"lat\"]],\n",
    "    exclude_dims={\"lat\"},  # dimensions allowed to change size. Must be set!\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs=dict(output_sizes={\"lat\": len(newlat)}),\n",
    "    output_dtypes=[ds.air.dtype],\n",
    "    vectorize=True,\n",
    ")\n",
    "interped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Again, it is important to understand the conceptual flow of information between the variuus packages when executing `interped.compute()` which looks ilke\n",
    "\n",
    "`xarray.apply_ufunc` ↔ `dask.array.apply_gufunc` ↔ `numpy.vectorize` ↔ `numpy.interp`\n",
    "\n",
    "\n",
    "When executed\n",
    "\n",
    "1. Xarray loops over all data variables.\n",
    "1. Xarray unwraps the underlying dask array (e.g. `ds.air`) and passes that to dask's `apply_gufunc`.\n",
    "1. `apply_gufunc` calls the vectorized function on each block of the array.\n",
    "1. For each block, `numpy.vectorize` handles looping over the loop dimensions \n",
    "   and passes 1D vectors along the core dimension to `numpy.interp`\n",
    "1. The 1D results for each block are concatenated by `numpy.vectorize` to create one output block.\n",
    "1. dask stitches all the output blocks to form the output array.\n",
    "1. `xarray.apply_ufunc` wraps the output array with Xarray metadata to give the final result.\n",
    "\n",
    "Phew!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "client.close();"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
