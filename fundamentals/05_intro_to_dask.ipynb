{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.xarray.dev/en/stable/_static/dataset-diagram-logo.png\" align=\"right\" width=\"30%\">\n",
    "\n",
    "# Introduction to Dask\n",
    "\n",
    "In this lesson, we discuss cover the basics of Dask. Our learning goals are as\n",
    "follows. By the end of the lesson, we will be able to:\n",
    "\n",
    "- Identify and describe Dask Collections (Array, DataFrame) and Schedulers\n",
    "- Work with Dask Array's in much the same way you would work with a NumPy array\n",
    "- Understand some of the tradeoffs surrounding chunk size, chunk shape, and\n",
    "  computational overhead\n",
    "- Deploy a local Dask Distributed Cluster and access the diagnostics dashboard\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [**What-is-Dask?**](#What-is-Dask?)\n",
    "1. [**Dask Collections**](#Dask-Collections)\n",
    "1. [**Parallelism using the dask.distributed scheduler**](#Parallelism-using-the-dask.distributed-scheduler)\n",
    "1. [**Profiling & Diagnostics using the Dask Dashboard**](#Profiling-&-Diagnostics-using-the-Dask-Dashboard)\n",
    "1. [**Distributed Dask clusters for HPC and Cloud environments**](#Distributed-Dask-clusters-for-HPC-and-Cloud-environments)\n",
    "\n",
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" \n",
    "     width=\"30%\" \n",
    "     align=right\n",
    "     alt=\"Dask logo\">\n",
    "\n",
    "## What is Dask?\n",
    "\n",
    "Dask is a flexible parallel computing library for analytic computing. Dask\n",
    "provides dynamic parallel task scheduling and high-level big-data collections\n",
    "like `dask.array` and `dask.dataframe`, and an extensive suite of deployment\n",
    "options. Dask's documentation can be found here:\n",
    "https://docs.dask.org/en/latest/\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-overview.svg\" \n",
    "     width=\"75%\" \n",
    "     align=center\n",
    "     alt=\"Dask overview\">\n",
    "\n",
    "## Quick setup\n",
    "\n",
    "For the purposes of this notebook, we'll use a Dask Cluster to manage\n",
    "computations. The next cell sets up a simple LocalCluster. We'll cover Dask\n",
    "schedulers and clusters later on in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&#128070</p> Click the Dashboard link above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Collections\n",
    "\n",
    "Dask includes 3 main collections:\n",
    "\n",
    "- [Dask Array](https://docs.dask.org/en/latest/array.html): Parallel NumPy\n",
    "  arrays\n",
    "- [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html): Parallel\n",
    "  Pandas DataFrames\n",
    "- [Dask Bag](https://docs.dask.org/en/latest/bag.html): Parallel Python Lists\n",
    "\n",
    "Xarray primarily interfaces with the Dask Array collection so we'll skip the\n",
    "others for now. You can find out more about Dask's user interfaces\n",
    "[here](https://docs.dask.org/en/latest/user-interfaces.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Arrays\n",
    "\n",
    "Dask Array implements a subset of the NumPy ndarray interface using blocked\n",
    "algorithms, cutting up the large array into many small arrays. This lets us\n",
    "compute on arrays larger than memory using multiple cores. We coordinate these\n",
    "blocked algorithms using Dask graphs. Dask Array's are also _lazy_, meaning that\n",
    "they do not evaluate until you explicitly ask for a result using the `compute`\n",
    "method.\n",
    "\n",
    "If we want to create a NumPy array of all ones, we do it like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shape = (1000, 4000)\n",
    "ones_np = np.ones(shape)\n",
    "ones_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array contains exactly 32 MB of data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.1f MB\" % (ones_np.nbytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the same array using Dask's array interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "ones = da.ones(shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but we didn't tell Dask how to split up (or chunk) the array, so it\n",
    "is not optimized for parallel computation.\n",
    "\n",
    "A crucal difference with Dask is that we must specify the `chunks` argument.\n",
    "\"Chunks\" describes how the array is split up over many sub-arrays.\n",
    "\n",
    "![Dask Arrays](http://docs.dask.org/en/latest/_images/dask-array-black-text.svg)\n",
    "_source:\n",
    "[Dask Array Documentation](http://docs.dask.org/en/latest/array-overview.html)_\n",
    "\n",
    "There are\n",
    "[several ways to specify chunks](http://docs.dask.org/en/latest/array-creation.html#chunks).\n",
    "In this lecture, we will use a block shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (1000, 1000)\n",
    "ones = da.ones(shape, chunks=chunk_shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we just see a symbolic represetnation of the array, including its\n",
    "shape, dtype, and chunksize. No data has been generated yet. When we call\n",
    "`.compute()` on a Dask array, the computation is trigger and the dask array\n",
    "becomes a numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what happened when we called `.compute()`, we can\n",
    "visualize the Dask _graph_, the symbolic operations that make up the array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our array has four chunks. To generate it, Dask calls `np.ones` four times and\n",
    "then concatenates this together into one array.\n",
    "\n",
    "Rather than immediately loading a Dask array (which puts all the data into RAM),\n",
    "it is more common to reduce the data somehow. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones = ones.sum()\n",
    "sum_of_ones.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Modify the chunk size (or shape) in the `ones` array and visualize how the task\n",
    "graph changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see Dask's strategy for finding the sum. This simple example illustrates\n",
    "the beauty of Dask: it automatically designs an algorithm appropriate for custom\n",
    "operations with big data.\n",
    "\n",
    "If we make our operation more complex, the graph gets more complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_calculation = (ones * ones[::-1, ::-1]).mean()\n",
    "fancy_calculation.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bigger Calculation\n",
    "\n",
    "The examples above were toy examples; the data (32 MB) is probably not big\n",
    "enough to warrant the use of Dask.\n",
    "\n",
    "We can make it a lot bigger!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigshape = (200000, 4000)\n",
    "big_ones = da.ones(bigshape, chunks=chunk_shape)\n",
    "big_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.1f MB\" % (big_ones.nbytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is 6.4 GB, rather than 32 MB! This is probably close to or greater\n",
    "than the amount of available RAM than you have in your computer. Nevertheless,\n",
    "Dask has no problem working on it.\n",
    "\n",
    "_Do not try to `.visualize()` this array!_\n",
    "\n",
    "When doing a big calculation, dask also has some tools to help us understand\n",
    "what is happening under the hood. Let's watch the dashboard again as we do a\n",
    "bigger computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_calc = (big_ones * big_ones[::-1, ::-1]).mean()\n",
    "\n",
    "result = big_calc.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction\n",
    "\n",
    "All the usual numpy methods work on dask arrays. You can also apply numpy\n",
    "function directly to a dask array, and it will stay lazy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ones_reduce = (np.cos(big_ones) ** 2).mean(axis=1)\n",
    "big_ones_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting also triggers computation, since we need the actual values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(big_ones_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism using the dask.distributed scheduler\n",
    "\n",
    "In the [first cell](#Quick-setup) of this notebook, we started a local Dask\n",
    "Cluster and Client. We skipped past some important details there that we'll\n",
    "unpack now.\n",
    "\n",
    "### Dask Schedulers\n",
    "\n",
    "The Dask _Schedulers_ orchestrate the tasks in the Task Graphs so that they can\n",
    "be run in parallel. _How_ they run in parallel, though, is determined by which\n",
    "_Scheduler_ you choose.\n",
    "\n",
    "There are 3 _local_ schedulers:\n",
    "\n",
    "- **Single-Thread Local:** For debugging, profiling, and diagnosing issues\n",
    "- **Multi-threaded:** Using the Python built-in `threading` package (the default\n",
    "  for all Dask operations except `Bags`)\n",
    "- **Multi-process:** Using the Python built-in `multiprocessing` package (the\n",
    "  default for Dask `Bags`)\n",
    "\n",
    "and 1 _distributed_ scheduler, which we will talk about later:\n",
    "\n",
    "- **Distributed:** Using the `dask.distributed` module (which uses `tornado` for\n",
    "  communication over TCP). The distributed scheduler uses a `Cluster` to manage\n",
    "  communication between the scheduler and the \"workers\". This is described in\n",
    "  the next section.\n",
    "\n",
    "### Distributed Clusters (http://distributed.dask.org/)\n",
    "\n",
    "- `LocalCluster` - Creates a `Cluster` that can be executed locally. Each\n",
    "  `Cluster` includes a `Scheduler` and `Worker`s.\n",
    "- `Client` - Connects to and drives computation on a distributed `Cluster`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling & Diagnostics using the Dask Dashboard\n",
    "\n",
    "You'll recall from above, that we opened a url to the Dask Dashboard:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard the Dask distributed scheduler provides a an incredibly valuable\n",
    "tool for gaining insights into the performance of your computation and the\n",
    "cluster as a whole. In the dashboard, you'll see a number of tags:\n",
    "\n",
    "- _Status_: Overview of the current state of the scheduler, including the active\n",
    "  task stream, progress, memory per worker, and the number of tasks per worker.\n",
    "- _Workers_: The workers tab allows you to track cpu and memory use per worker.\n",
    "- _System_: Live tracking of system resources like cpu, memory, bandwidth, and\n",
    "  open file descriptors\n",
    "- _Profile_: Fine-grained statistical profiling\n",
    "- _Info_: Worker status and logs.\n",
    "\n",
    "Another useful diagnostic tool is Dask's static performance report. This allows\n",
    "you to save a report, including the task stream, worker profiles, etc. for all\n",
    "or a specific part of a workflow. Below is an example of how you would create\n",
    "such a report:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import performance_report\n",
    "\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    big_calc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Again, let's modify the chunk size in `big_ones` (aim for ~100mb). How does the\n",
    "_Performance Report_ change with a larger chunk size?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "with performance_report(filename=\"dask-report-large-chunk.html\"):\n",
    "    big_calc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Dask clusters for HPC and Cloud environments\n",
    "\n",
    "Dask can be deployed on distributed infrastructure, such as a an HPC system or a\n",
    "cloud computing system. There is a growing ecosystem of Dask deployment projects\n",
    "that facilitate easy deployment and scaling of Dask clusters on a wide variety of\n",
    "computing systems.\n",
    "\n",
    "### HPC\n",
    "\n",
    "#### Dask Jobqueue (https://jobqueue.dask.org/)\n",
    "\n",
    "- `dask_jobqueue.PBSCluster`\n",
    "- `dask_jobqueue.SlurmCluster`\n",
    "- `dask_jobqueue.LSFCluster`\n",
    "- etc.\n",
    "\n",
    "#### Dask MPI (https://mpi.dask.org/)\n",
    "\n",
    "- `dask_mpi.initialize`\n",
    "\n",
    "### Cloud\n",
    "\n",
    "#### Dask Kubernetes (https://kubernetes.dask.org/)\n",
    "\n",
    "- `dask_kubernetes.KubeCluster`\n",
    "\n",
    "#### Dask Cloud Provider (https://cloudprovider.dask.org)\n",
    "\n",
    "- `dask_cloudprovider.FargateCluster`\n",
    "- `dask_cloudprovider.ECSCluster`\n",
    "- `dask_cloudprovider.ECSCluster`\n",
    "\n",
    "#### Dask Gateway (https://gateway.dask.org/)\n",
    "\n",
    "- `dask_gateway.GatewayCluster`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_Note: Pieces of this notebook comes from the following sources:_\n",
    "\n",
    "- https://github.com/pangeo-data/pangeo-tutorial\n",
    "- https://github.com/rabernat/research_computing\n",
    "- https://github.com/dask/dask-examples\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
